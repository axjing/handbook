{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 环境变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/workspaces/trainX/notebooks', '/home/codespace/.python/current/lib/python310.zip', '/home/codespace/.python/current/lib/python3.10', '/home/codespace/.python/current/lib/python3.10/lib-dynload', '', '/home/codespace/.local/lib/python3.10/site-packages', '/home/codespace/.python/current/lib/python3.10/site-packages', '/home/test', '/home/test']\n",
      "['/workspaces/trainX/notebooks', '/home/codespace/.python/current/lib/python310.zip', '/home/codespace/.python/current/lib/python3.10', '/home/codespace/.python/current/lib/python3.10/lib-dynload', '', '/home/codespace/.local/lib/python3.10/site-packages', '/home/codespace/.python/current/lib/python3.10/site-packages', '/home/test', '/home/test', '/home/test']\n",
      "-1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "# 环境变量\n",
    "env_v=sys.path\n",
    "print(env_v)\n",
    "# 添加环境变量\n",
    "path='/home/test'\n",
    "add_env_v=sys.path.append(path)\n",
    "print(sys.path)\n",
    "\n",
    "print(os.getenv('work',-1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分组卷积工作原理\n",
    "Group convolution（分组卷积）的工作原理涉及将输入和输出通道分成若干组，以便对每组进行独立的卷积操作。以下是分组卷积的基本工作原理：\n",
    "\n",
    "1. **通道分组：** 首先，输入通道和输出通道被划分为 \\(G\\) 组，其中 \\(G\\) 是分组数。每个组内有相等数量的输入通道和输出通道。例如，如果输入通道数是 \\(C_{\\text{in}}\\)，输出通道数是 \\(C_{\\text{out}}\\)，而分组数是 \\(G\\)，则每个分组内的通道数量为 \\(C_{\\text{in}}/G\\) 和 \\(C_{\\text{out}}/G\\)。\n",
    "\n",
    "2. **独立卷积操作：** 对于每个分组，分别使用相应的输入通道组和输出通道组执行卷积操作。每组都有自己的卷积核集合，这些卷积核仅与相应的输入通道组相关联。这意味着每个分组都有自己独立的卷积过程，独立生成输出通道组。\n",
    "\n",
    "3. **结果拼接：** 将每个分组产生的输出通道组合并成最终的输出。这个过程涉及将每个分组的输出通道按照通道维度进行拼接，形成最终的输出特征图。\n",
    "\n",
    "这种方式使得每个分组内的通道只与同组内的通道进行卷积操作，而不与其他分组的通道进行卷积。这可以在一定程度上减少模型的参数量，并且提高计算效率。\n",
    "\n",
    "下面是一个简单的伪代码表示，说明了分组卷积的基本工作原理：\n",
    "\n",
    "```python\n",
    "# 伪代码示例\n",
    "for each_group in range(groups):\n",
    "    # 从输入通道和输出通道中选择相应的通道组\n",
    "    input_channel_group = input_channels[each_group * (C_in // groups) : (each_group + 1) * (C_in // groups)]\n",
    "    output_channel_group = output_channels[each_group * (C_out // groups) : (each_group + 1) * (C_out // groups)]\n",
    "\n",
    "    # 使用相应的通道组进行独立的卷积操作\n",
    "    output_group = convolve(input_channel_group, output_channel_group, kernel_size, stride, padding)\n",
    "\n",
    "    # 存储每个分组的输出\n",
    "\n",
    "# 将每个分组的输出拼接成最终的输出特征图\n",
    "final_output = concatenate(output_group_1, output_group_2, ..., output_group_G)\n",
    "```\n",
    "\n",
    "这就是分组卷积的基本工作原理，通过对通道进行分组，实现了部分独立卷积，从而减少了参数量。\n",
    "\n",
    "分组卷积（Group Convolution）是通过将输入和输出通道划分为若干组，每组内的通道独立进行卷积操作来实现的。这样，卷积核也被分成了若干组，每组与相应的输入通道组相连，从而完成卷积运算。下面是分组卷积的公式和方法的简要描述：\n",
    "\n",
    "### 公式：\n",
    "\n",
    "对于输入特征图 \\(X\\) 和输出特征图 \\(Y\\)，以及分组卷积的参数（卷积核权重） \\(W\\)，分组卷积的输出可以表示为：\n",
    "\n",
    "$$\n",
    "Y_{i,j,k} = \\sum_{l=0}^{C_{\\text{in}}/G - 1} \\sum_{m=0}^{H_{\\text{f}} - 1} \\sum_{n=0}^{W_{\\text{f}} - 1} X_{i,l \\cdot G + m,j \\cdot S + n} \\cdot W_{k,l,m,n}\n",
    "$$\n",
    "\n",
    "其中：\n",
    "\n",
    "- \\(Y_{i,j,k}\\) 是输出特征图的第 \\(i\\) 个样本的第 \\(j\\) 个通道的第 \\(k\\) 个元素。\n",
    "- \\(X_{i,c,h,w}\\) 是输入特征图的第 \\(i\\) 个样本的第 \\(c\\) 个通道的第 \\(h\\) 行、第 \\(w\\) 列的元素。\n",
    "- \\(W_{k,c,m,n}\\) 是第 \\(k\\) 个输出通道的第 \\(c\\) 个输入通道的卷积核的第 \\(m\\) 行、第 \\(n\\) 列的权重。\n",
    "- \\(C_{\\text{in}}\\) 是输入通道数，\\(H_{\\text{f}}\\) 和 \\(W_{\\text{f}}\\) 是卷积核的高度和宽度，\\(S\\) 是卷积的步幅，\\(G\\) 是分组数。\n",
    "\n",
    "### 方法：\n",
    "\n",
    "1. **分组划分：** 将输入和输出通道分成 \\(G\\) 组，确保输入通道数和输出通道数都是 \\(G\\) 的整数倍。\n",
    "\n",
    "2. **分组卷积操作：** 对于每个分组，独立地对输入通道和输出通道进行卷积操作。这就意味着每个分组有自己的卷积核集合，用于处理相应的输入通道和生成相应的输出通道。\n",
    "\n",
    "3. **拼接结果：** 将各个分组的卷积结果进行拼接，得到最终的输出特征图。\n",
    "\n",
    "在PyTorch中，通过设置`groups`参数来实现分组卷积。例如，`nn.Conv2d(in_channels=..., out_channels=..., kernel_size=..., groups=...)`中的`groups`参数即用于指定分组卷积的分组数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "print(f\"显卡是否可用{torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"显卡信息：{torch.cuda.get_device_name()}\")\n",
    "    device = torch.device(\"cuda\")\n",
    "    memory_size = torch.cuda.get_device_properties(device).total_memory\n",
    "    print(\"显存大小:\", memory_size)\n",
    "    frequency = torch.cuda.get_device_properties(device)#.clock_rate\n",
    "    print(\"核心频率:\", frequency)\n",
    "# 普通卷积\n",
    "conv1 = nn.Conv2d(in_channels=3, out_channels=6, kernel_size=3, stride=1, padding=1)\n",
    "print(conv1)\n",
    "\n",
    "# 分组卷积，每个输入通道独立卷积，输出通道数也是输入通道数\n",
    "conv2 = nn.Conv2d(in_channels=3, out_channels=6, kernel_size=3, stride=1, padding=1, groups=3)\n",
    "print(conv2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 防止出现路径错误问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpathlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[0;32m----> 4\u001b[0m FILE \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;18;43m__file__\u001b[39;49m)\u001b[38;5;241m.\u001b[39mresolve()\n\u001b[1;32m      5\u001b[0m ROOT \u001b[38;5;241m=\u001b[39m FILE\u001b[38;5;241m.\u001b[39mparents[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# YOLOv5 root directory\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(ROOT) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mpath:\n",
      "\u001b[0;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "FILE = Path(__file__).resolve()\n",
    "ROOT = FILE.parents[0]  # YOLOv5 root directory\n",
    "if str(ROOT) not in sys.path:\n",
    "    sys.path.append(str(ROOT))  # add ROOT to PATH\n",
    "ROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# platform 判断平台"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linux\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "\n",
    "MACOS, LINUX, WINDOWS = (platform.system() == x for x in ['Darwin', 'Linux', 'Windows'])  # environment booleans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 路径处理\n",
    "## pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_qwertyuiop0\n",
      "/root/home/user1/test_qwertyuiop0_openvino_model/\n",
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "file_path='/root/home/user1/test_qwertyuiop0.png'\n",
    "f=Path(file_path)\n",
    "f1 = str(f).replace(f.suffix, f'_openvino_model{os.sep}')\n",
    "print(f.stem)\n",
    "print(f1)\n",
    "print(isinstance(file_path,str))\n",
    "print('.png' in file_path)\n",
    "print('.png1' in file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 字符串处理\n",
    "## replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "net.state.module.layer1\n",
      "net.stata1.m.layer1\n"
     ]
    }
   ],
   "source": [
    "s1=\"net.state.module.layer1\"\n",
    "s2=\"net.stata1.m.layer1\"\n",
    "\n",
    "s1_o=s1.replace(\"moudle.\",\"\")\n",
    "s2_o=s2.replace(\"module.\",\"\")\n",
    "print(s1_o)\n",
    "print(s2_o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lambda函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[64, 128, 256, 512, 1024, 2048]\n",
      "<generator object <genexpr> at 0x7f43eb3229d0>\n",
      "(1, 1, 1)\n",
      "/model_name/UNet_Fold0\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "# list\n",
    "root_featmap_nums = 64\n",
    "featmaps = [root_featmap_nums*2**i for i in range(6)]\n",
    "print(featmaps)\n",
    "# tuple\n",
    "print((1 for i in range(2)))\n",
    "print(tuple([1 for i in range(3)]))\n",
    "# path\n",
    "save_ckpt = os.path.join(\"/\", \"model_name\", \"{}_Fold{}\".format(\"UNet\", 0))\n",
    "print(save_ckpt)\n",
    "# if\n",
    "internal_channels=8\n",
    "internal_channels= 1 if internal_channels==0 else internal_channels\n",
    "print(internal_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pytorch常用函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4, 4])\n",
      "tensor([[[1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# torch.cat\n",
    "a=[]\n",
    "for i in range(4):\n",
    "    a.append(torch.ones(1,4,4))\n",
    "a_=torch.cat(a,dim=0)\n",
    "print(a_.shape)\n",
    "print(a_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 装饰器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Something is happening before the function is called.\n",
      "Hello!\n",
      "Something is happening after the function is called.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-05 02:47:53,134 - INFO - Executing add_numbers with arguments (3, 5) and keyword arguments {}.\n",
      "2024-01-05 02:47:53,135 - INFO - add_numbers executed successfully. Result: 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function executed.\n",
      "example_function took 2.0009446144104004 seconds to run.\n"
     ]
    }
   ],
   "source": [
    "def my_decorator(func):\n",
    "    def wrapper():\n",
    "        print(\"Something is happening before the function is called.\")\n",
    "        func()\n",
    "        print(\"Something is happening after the function is called.\")\n",
    "    return wrapper\n",
    "\n",
    "@my_decorator\n",
    "def say_hello():\n",
    "    print(\"Hello!\")\n",
    "\n",
    "say_hello()\n",
    "import time\n",
    "\n",
    "def timing_decorator(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        print(f\"{func.__name__} took {end_time - start_time} seconds to run.\")\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "@timing_decorator\n",
    "def example_function():\n",
    "    # Some time-consuming task\n",
    "    time.sleep(2)\n",
    "    print(\"Function executed.\")\n",
    "\n",
    "example_function()\n",
    "\n",
    "import logging\n",
    "from functools import wraps\n",
    "\n",
    "def logging_decorator(func):\n",
    "    \"\"\"\n",
    "    Decorator that logs information about the function's execution.\n",
    "    \n",
    "    Parameters:\n",
    "    - func: The function to be decorated.\n",
    "    \n",
    "    Returns:\n",
    "    The decorated function.\n",
    "    \"\"\"\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        logging.info(f\"Executing {func.__name__} with arguments {args} and keyword arguments {kwargs}.\")\n",
    "        result = func(*args, **kwargs)\n",
    "        logging.info(f\"{func.__name__} executed successfully. Result: {result}\")\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "# 配置日志记录\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# 使用装饰器\n",
    "@logging_decorator\n",
    "def add_numbers(a, b):\n",
    "    \"\"\"\n",
    "    Example function that adds two numbers.\n",
    "    \"\"\"\n",
    "    return a + b\n",
    "\n",
    "# 调用被装饰的函数\n",
    "result = add_numbers(3, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# “*”作为函数参数的作用\n",
    "## 收集任意数量的位置参数 `*args`\n",
    "如果你想要一个函数能够接受任意数量的位置参数，你可以使用一个带有星号的参数，但是这个星号后面必须有一个参数名。\n",
    "\n",
    "这里的`*args`表示收集任意数量的位置参数，并将它们存储在元组 `args` 中。这样，你可以在函数体内循环访问这些参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'> (1, 2, 3, 5, 5, 6, 3)\n",
      "1\n",
      "2\n",
      "3\n",
      "5\n",
      "5\n",
      "6\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "def star_args_usage(*args):\n",
    "    print(type(args),args)\n",
    "    for i in args:\n",
    "        print(i)\n",
    "star_args_usage(1,2,3,5,5,6,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 指定关键字参数 \n",
    "测试函数中单独一个“*”的作用：\n",
    "如果你想在函数定义中指定一些必须使用关键字传递的参数，可以使用带有星号的参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3, 4)\n",
      "(1, 2, 2, 3)\n"
     ]
    }
   ],
   "source": [
    "def star_usage(*,a,b,c=3):\n",
    "    \"\"\"_summary_\n",
    "    测试函数中单独一个“*”的作用：\n",
    "    在你提供的函数签名中，*（单独的星号）的作用是指定在此之后的参数必须使用关键字传递，而不能使用位置传递。这样的语法用于强制调用者在调用函数时通过关键字明确指定某些参数的值。\n",
    "    Args:\n",
    "        a (int, optional): _description_. Defaults to 1.\n",
    "        b (int, optional): _description_. Defaults to 2.\n",
    "        c (int, optional): _description_. Defaults to 3.\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    return a,b,c\n",
    "def star_usage1(a,b,*,c,d):\n",
    "    \"\"\"\n",
    "    在这个例子中,a 和 b 是位置参数，而 c 和 d 必须使用关键字传递。这是因为在星号后的参数只能通过关键字传递。\n",
    "    \"\"\"\n",
    "    return a,b,c,d\n",
    "print(star_usage(a=2,b=3,c=4))\n",
    "print(star_usage1(1,2,c=2,d=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 解包可迭代对象\n",
    "如果在函数调用时使用星号（*），则表示将一个可迭代对象（如列表或元组）解包为单独的位置参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2 3\n"
     ]
    }
   ],
   "source": [
    "def example_function(a, b, c):\n",
    "    print(a, b, c)\n",
    "\n",
    "my_list = [1, 2, 3]\n",
    "\n",
    "example_function(*my_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **kwags\n",
    "在Python中，`**kwargs` 是一种用于函数定义的语法，表示函数可以接受任意数量的关键字参数，并将这些参数收集到一个字典中。\"kwargs\" 通常是 \"keyword arguments\" 的缩写。\n",
    "1. 接受任意数量的关键字参数： `**kwargs` 允许函数接受不定数量的关键字参数。这样，函数的调用者可以传递任意数量的关键字参数，而函数将它们作为一个字典进行处理。\n",
    "2. 与其他参数一起使用： `**kwargs` 可以与其他参数一起使用，但通常放在参数列表的末尾。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'> {'a': 1, 'b': 2}\n",
      "a\n",
      "b\n",
      "<class 'dict'> {'a': 1, 'b': 3, 'c': 2}\n",
      "a\n",
      "b\n",
      "c\n",
      "<class 'dict'> {'a': 1, 'b': 2}\n",
      "a\n",
      "b\n",
      "<class 'dict'> {'a': 1, 'b': 3, 'c': 2}\n",
      "a\n",
      "b\n",
      "c\n"
     ]
    }
   ],
   "source": [
    "def sstar_args_usage(**kwargs):\n",
    "    print(type(kwargs),kwargs)\n",
    "    for i in kwargs:\n",
    "        print(i)\n",
    "\n",
    "def sstar_args_usage1(x,y,**kwargs):\n",
    "    print(type(kwargs),kwargs)\n",
    "    for i in kwargs:\n",
    "        print(i)\n",
    "        \n",
    "sstar_args_usage(a=1,b=2)\n",
    "kwargs={'a':1,'b':3,'c':2}\n",
    "sstar_args_usage(**kwargs)\n",
    "\n",
    "sstar_args_usage1(-1,0,a=1,b=2)\n",
    "sstar_args_usage1(-1,0,**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `**variable`解包字典可迭代对象 \n",
    "\n",
    "在Python中，**variable（variable可以是任意合法的变量名）作为函数的参数传递时，它的作用是将字典中的键值对解包为关键字参数，其中字典是由变量 variable 所引用的。\n",
    "\n",
    "具体来说，如果一个函数的定义中使用了 **variable，那么在调用该函数时，可以将一个字典传递给这个变量，而字典中的键值对将会被解包为关键字参数。这种方式使得在函数调用时能够动态地传递参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name John\n",
      "age 25\n",
      "city New York\n",
      "1 2 3\n"
     ]
    }
   ],
   "source": [
    "def example_function(**kwargs):\n",
    "    for key, value in kwargs.items():\n",
    "        print(key, value)\n",
    "\n",
    "def exp2(a,b,c):\n",
    "    print(a,b,c)\n",
    "# 定义一个字典\n",
    "my_dict = {'name': 'John', 'age': 25, 'city': 'New York'}\n",
    "\n",
    "# 将字典解包为关键字参数\n",
    "example_function(**my_dict)\n",
    "vari={'a':1,'b':2,'c':3}\n",
    "exp2(**vari)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这个例子中，example_function 接受字典中的键值对作为关键字参数，并将它们打印出来。使用 **my_dict 的语法将字典解包，使得函数能够动态处理字典中的内容。\n",
    "\n",
    "需要注意的是，传递的字典中的键必须与函数定义中的参数名相匹配，否则会引发 TypeError。这种方式常用于编写通用函数，特别是在处理配置参数、动态生成函数参数等场景下。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# thop 库\n",
    "`thop`（Torch Operation Counter）是一个用于统计PyTorch模型中浮点运算数（FLOPs）和参数数量的库。FLOPs是模型进行推理时执行的浮点运算的数量，通常用于衡量模型的计算复杂度。参数数量是模型中需要学习的权重和偏置的数量。\n",
    "\n",
    "`thop` 提供了一个轻量级的工具，可以在不执行实际的前向传播的情况下，仅通过分析模型的计算图（图的结构）来估计模型的计算复杂度。这对于模型设计和优化非常有用，因为可以在模型构建之前就大致了解模型的计算和内存需求。\n",
    "\n",
    "以下是使用 `thop` 库的基本步骤：\n",
    "\n",
    "1. 安装 `thop` 库：`pip install thop`\n",
    "\n",
    "2. 导入库：`from thop import profile`\n",
    "\n",
    "3. 定义一个输入张量，通常是模型的输入大小。\n",
    "\n",
    "4. 使用 `profile` 函数来获取模型的计算复杂度和参数数量。\n",
    "\n",
    "以下是一个简单的示例：\n",
    "在这个例子中，`profile` 函数分析了 `SimpleModel` 的计算图，估计了前向传播时的FLOPs和参数数量，并将其打印出来。这对于理解模型的计算复杂度和优化模型非常有帮助。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "torch.Size([1, 64, 30, 30])\n",
      "torch.Size([1, 57600])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x57600 and 64x900)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m input_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m32\u001b[39m)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# 使用 thop 的 profile 函数\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m flops, params \u001b[38;5;241m=\u001b[39m \u001b[43mprofile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFLOPs: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mflops\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparams\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/thop/profile.py:212\u001b[0m, in \u001b[0;36mprofile\u001b[0;34m(model, inputs, custom_ops, verbose, ret_layer_info, report_missing)\u001b[0m\n\u001b[1;32m    209\u001b[0m model\u001b[38;5;241m.\u001b[39mapply(add_hooks)\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 212\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdfs_count\u001b[39m(module: nn\u001b[38;5;241m.\u001b[39mModule, prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m (\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m):\n\u001b[1;32m    215\u001b[0m     total_ops, total_params \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39mtotal_ops\u001b[38;5;241m.\u001b[39mitem(), \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[14], line 17\u001b[0m, in \u001b[0;36mSimpleModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     15\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(x\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 17\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(x\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/torch/nn/modules/module.py:1568\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1565\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m hooks\u001b[38;5;241m.\u001b[39mBackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1566\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1568\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1569\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1570\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   1571\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1572\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1573\u001b[0m     ):\n\u001b[1;32m   1574\u001b[0m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x57600 and 64x900)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from thop import profile\n",
    "\n",
    "class SimpleModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(3, 64, kernel_size=3)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.fc = torch.nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        print(x.shape)\n",
    "        # x = x.view(x.size(0), -1)\n",
    "        # print(x.shape)\n",
    "        # x = self.fc(x)\n",
    "        # print(x.shape)\n",
    "        return x\n",
    "\n",
    "# 创建模型实例\n",
    "model = SimpleModel()\n",
    "\n",
    "# 定义输入大小\n",
    "input_tensor = torch.randn(1, 3, 32, 32)\n",
    "\n",
    "# 使用 thop 的 profile 函数\n",
    "flops, params = profile(model, inputs=(input_tensor,))\n",
    "\n",
    "print(f\"FLOPs: {flops}\")\n",
    "print(f\"Params: {params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1920"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "30*64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
